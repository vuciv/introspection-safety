\section{Experiments \& Results}

We evaluated the effectiveness of our supervised fine-tuning approach across three dimensions: (1) the ability to learn introspective reporting on trained concepts, (2) the generalization of this capability to unseen concepts, and (3) the robustness of the mechanism against hallucination and random noise.

\subsection{Experiment 1: Learning to Introspect}
We first established a baseline by comparing the performance of the base \texttt{DeepSeek-7B} model against our fine-tuned (LoRA) model on the set of 38 concepts seen during training. 

The base model, lacking specific training for this task, failed completely. It achieved a sensitivity (True Positive Rate) of only 31\%, comparable to random guessing, and a specificity (True Negative Rate) of 0\%, indicating it hallucinated "injected thoughts" on every control trial. In contrast, the fine-tuned model achieved a \textbf{88.5\% sensitivity} and \textbf{100\% specificity}. This result confirms that while introspection is not emergent in 7B models, it is highly amenable to supervised learning.

\subsection{Experiment 2: Generalization to Unseen Concepts}
A critical question is whether the model learned a genuine introspective mechanism (i.e., a "decoder" for the residual stream) or simply memorized a mapping between specific activation vectors and text outputs (e.g., Vector A $\rightarrow$ "Spider").

To test this, we evaluated the models on the held-out set of 20 concepts (e.g., "origami", "tornado") which were never seen during training. 
\begin{itemize}
    \item \textbf{Base Model:} Performance degraded further to 16\% sensitivity.
    \item \textbf{Fine-Tuned Model:} The model maintained a high sensitivity of \textbf{71\%} on these unseen concepts.
\end{itemize}
This strong generalization gap implies that the fine-tuning process instantiated a general-purpose circuit capable of mapping arbitrary semantic directions in the residual stream to their corresponding linguistic tokens, rather than merely overfitting to the training set.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/generalization_gap.png}
    \caption{\textbf{Introspection Generalizes to Unseen Concepts.} The fine-tuned model (green) maintains high accuracy on concepts it never saw during training, whereas the base model (grey) performs poorly. Error bars indicate standard error.}
    \label{fig:generalization}
\end{figure}

\subsection{Experiment 3: Robustness and Hallucination}
A common failure mode of "introspective" models is confabulationâ€”inventing internal states to satisfy the user's prompt. We tested robustness using two controls:

\textbf{Control A: The Empty Stream.} On trials with no injection (Strength=0), the base model had a 100\% False Positive rate, always claiming to detect a thought. The fine-tuned model achieved a perfect \textbf{0\% False Positive rate}, correctly reporting "I do not detect any injected thoughts" in 50/50 trials.

\textbf{Control B: The Random Vector Test.} To ensure the model wasn't simply triggering on \textit{any} anomaly, we injected random noise vectors normalized to the same magnitude as concept vectors.
\begin{itemize}
    \item \textbf{Result:} The fine-tuned model did \textit{not} hallucinate specific training concepts (e.g., "spider"). Instead, at low strengths (0-40), it correctly reported "No detection." At high strengths (60-100), it reported "Anomaly detected" or generic concepts (e.g., "something strange"), but crucially, it achieved a \textbf{0\% rate of specific hallucinations}.
\end{itemize}
This confirms the model behaves as a calibrated instrument: it reports specific concepts only when the specific semantic signal is present.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Seen Concepts}} & \multicolumn{2}{c}{\textbf{Unseen Concepts}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        \textbf{Model} & Sensitivity & Specificity & Sensitivity & Specificity \\
        \midrule
        Base Model & 0.31 & 0.00 & 0.16 & 0.00 \\
        \textbf{Fine-Tuned (Ours)} & \textbf{0.89} & \textbf{1.00} & \textbf{0.71} & \textbf{1.00} \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison. Specificity refers to the True Negative Rate (avoiding hallucinations).}
    \label{tab:results}
\end{table}

