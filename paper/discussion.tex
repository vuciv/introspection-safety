\section{Discussion}

Our findings challenge the prevailing view that introspection in Large Language Models is a fragile, emergent property reserved for the largest models. By demonstrating that a 7B parameter model can learn to robustly report its internal states, we suggest that introspective awareness is better understood as a \textit{skill}â€”one that models possess the latent capacity for, but lack the vocabulary to express without targeted supervision.

\subsection{The "Decoder" Mechanism}
The most significant implication of our work is the strong generalization to unseen concepts (Experiment 2). If the model were simply memorizing associations (e.g., learning that a specific activation pattern means "spider"), it would fail entirely on novel vectors like "origami." The fact that it succeeds suggests the model has learned a general-purpose "decoder" circuit. This circuit likely learns to attend to the semantic content of the residual stream and translate it into the corresponding token in the vocabulary space. This "translation layer" effectively bridges the gap between the model's latent representation space and its output space, allowing it to "speak" its own mind.

\subsection{Implications for AI Safety and Honesty}
Current alignment techniques (RLHF) train models to produce outputs that \textit{look} honest to human evaluators. However, this can incentivize deceptive behavior if the model's internal state diverges from its output (e.g., a model that "knows" it is lying but outputs a confident falsehood). 
Our results demonstrate a proof-of-concept for "Internal State Reporting." If we can train models to reliably report the presence of simple concepts like "apple" or "fear," we may be able to scale this technique to more complex internal states, such as "deception," "uncertainty," or "goal conflict." An introspective model could act as its own safety monitor, flagging potentially harmful internal states before they manifest in external behavior.

\subsection{Limitations and Future Work}
Our work relies on synthetic "injected thoughts," which act as a proxy for natural model activations. While valid for establishing the learnability of the mechanism, future work must verify whether this capability transfers to naturally occurring thoughts (e.g., catching a model in the act of hallucination without injection). Additionally, further research is needed to interpret the mechanistic circuits responsible for this decoding capability.

\section{Conclusion}
We presented a method for robustly eliciting introspective awareness in open-weights LLMs via supervised fine-tuning. We showed that a 7B model can learn to accurately identifying injected concept vectors, generalize this ability to unseen concepts, and avoid hallucination on control trials. These results suggest that introspection is a robust, learnable capability that can be harnessed to create more transparent and trustworthy AI systems.

